物联网对抗性网络行为判别
======

## 项目说明

### 题目分析

这是一道非常经典的表格类题目，题目的数据集包含物联网网络流量的各种特征和相应的攻击类型标签。每行数据表示一次网络流量记录，包含多种流量特征，如协议类型、流持续时间、数据包数量、窗口大小等。

题目的主要任务是开发一个能够识别和防御网络流量攻击的有效模型。通过提供的网络流量数据集，我们需要对数据进行预处理、特征工程、模型训练和评估。

数据预处理方面，考虑到数据集没有缺失值且数值特征已标准化，我们不需要额外的处理。特征工程方面，我们删除了无用的`id.orig_p`列，并对`id.resp_p`列进行了处理，仅保留最常见的端口特征，简化数据的同时保留重要信息。

模型训练方面，选择了`TabNet`模型，这是一种结合决策树和神经网络优点的深度学习模型，特别适合处理表格数据。它具备良好的可解释性和处理稀疏特征的能力，并能在较少数据上进行有效训练。

通过将数据集按`9:1`的比例分割为训练集和测试集，我们可以训练并评估模型，使用准确率和损失值作为评价指标，确保模型在实际应用中的有效性。

本项目通过对物联网网络流量数据的处理和分析，利用`TabNet`模型识别网络攻击类型，从而提高物联网设备的安全性。

### 数据处理方法

1. **数据预处理**
   - 题目的数据集没有出现缺失值的情况，不需要进行缺失值处理。
   - 题目的数值特征都已经进行过标准化和归一化了，也不需要进行处理。
2. **特征工程**
   - 题目的`id.orig_p`列看似是一串随机数，对结果分析没有任何帮助，可以删除。
   - 题目的`id.resp_p`列看似是表示端口的数据，而且有许多重复的端口，为此我选择出现次数最多的10个端口作为特征，其余的端口数据全部置零。
3. **训练集分割**
   - 将数据集按照`9：1`的比例切割训练集和测试集，用于模型的训练和评估。
   - 由于数据量较大且充分，因此不需要额外的交叉特征处理。
4. **模型评估**
   - 使用测试数据评估模型性能。
   - 由于是分类问题，只需要用传统的准确率和损失值评估模型即可。

### 技术路线

主要走`TabNet`模型，实现决策树和神经网络相结合的技术路线。选择`tensorflow`作为深度学习框架，用`pandas`库来读取题目给的`csv`格式文件。

### 算法框架

我走的算法框架是经典的`TabNet`网络模型。`TabNet` 是一个用于处理表格数据的深度学习模型，结合了决策树和神经网络的优点，具有一些显著的优势。以下是 `TabNet` 模型的主要优点：

#### 1. **可解释性**

`TabNet` 通过特征选择模块提供了每个决策步骤的可解释性。模型可以指出在每个决策步骤中哪些特征被使用了，以及它们对最终预测的贡献。这使得 `TabNet` 不仅能提供预测结果，还能解释这些结果是如何得出的。

#### 2. **处理稀疏特征**

`TabNet` 在处理稀疏特征（即大量特征中大多数值为零的情况）时表现良好。它的架构设计可以有效地选择和利用重要特征，而忽略不重要的特征，从而提高模型的性能和训练效率。

#### 3. **端到端学习**

`TabNet` 可以直接从原始数据中学习，无需复杂的预处理步骤。这意味着你可以将原始表格数据直接输入模型，而不需要进行过多的特征工程。

#### 4. **嵌入层**

`TabNet` 使用嵌入层将类别特征转化为高维向量表示，类似于在自然语言处理中的词嵌入。这使得模型可以更好地处理类别特征，并提高预测精度。

#### 5. **梯度提升与深度学习的结合**

`TabNet` 结合了梯度提升树的可解释性和深度学习的强大表示能力。这使得模型在处理复杂的非线性关系时表现出色，同时保持了一定程度的可解释性。

#### 6. **数据高效**

`TabNet` 可以在相对较少的数据量上进行有效训练。其架构设计使得模型在有限的数据集上也能表现良好。

#### 7. **并行计算**

`TabNet` 的设计允许高效的并行计算，这使得它在大规模数据集上具有较高的训练速度和预测速度。

#### 8. **灵活性**

`TabNet` 可以处理不同类型的表格数据，包括分类、回归、多分类等任务。此外，它还可以处理时间序列数据。

由于题目给出的表格数据特征已经比较丰富，所以不需要额外再进行特征工程，直接用题目给出的数据带入到`TabNet`模型里训练即可。

## 环境配置

### 软件环境

`python3.9`

所需要的库已经写在`requirements.txt`里面了，输入以下命令安装

```bash
pip install -r requirements.txt
```

推荐使用`Anaconda`创建虚拟环境

### 硬件环境

`CPU`

	13th Gen Intel(R) Core(TM) i7-13700H
	
	基准速度:	2.40 GHz
	插槽:	1
	内核:	14
	逻辑处理器:	20
	虚拟化:	已启用
	L1 缓存:	1.2 MB
	L2 缓存:	11.5 MB
	L3 缓存:	24.0 MB

`GPU`

	NVIDIA GeForce RTX 4060 Laptop GPU
	
	驱动程序版本:	31.0.15.5161
	驱动程序日期:	2024/2/15
	DirectX 版本:	12 (FL 12.1)
	物理位置：	PCI 总线 1、设备 0、功能 0
	
	利用率	0%
	专用 GPU 内存	0.0/8.0 GB
	共享 GPU 内存	0.0/15.8 GB
	GPU 内存	0.0/23.8 GB

内存

	32.0 GB
	
	速度:	4800 MHz
	已使用的插槽:	2/2
	外形规格:	SODIMM

## 模型设置

在`源码`文件夹下方有一个`params.py`文件，存放了以下参数

```python
"decision_dim": 16,
"attention_dim": 16,
"n_steps": 3,
"n_shared_glus": 2,
"n_dependent_glus": 2,
"relaxation_factor": 1.5,
"epsilon": 1e-15,
"virtual_batch_size": None,
"momentum": 0.98,
"mask_type": "entmax",
"lambda_sparse": 1e-4,
```

| 参数                 | 说明                                                         |
| -------------------- | ------------------------------------------------------------ |
| `decision_dim`       | 每一步的决策维度，决定了每一步输出的特征数量。               |
| `attention_dim`      | 每一步的注意力维度，用于特征选择和决策生成。                 |
| `n_steps`            | 决策步骤数，表示模型在每个输入上进行多少次决策。             |
| `n_shared_glus`      | 共享`GLU`层的数量，这些层在所有决策步骤之间共享。            |
| `n_dependent_glus`   | 依赖`GLU`层的数量，这些层在每个决策步骤中独立。              |
| `relaxation_factor`  | 松弛因子，用于控制每一步特征选择的松弛程度。                 |
| `epsilon`            | 防止数值不稳定的小常数，通常用于避免分母为零的情况。         |
| `virtual_batch_size` | 虚拟批次大小，用于虚拟批次规范化，如果未设置则不使用虚拟批次规范化。 |
| `momentum`           | `BatchNorm`中的动量参数，控制均值和方差的更新速度。          |
| `mask_type`          | 掩码类型，决定特征选择机制，常用值为`"entmax"`或`"sparsemax"`。 |
| `lambda_sparse`      | 稀疏性正则化系数，用于控制特征选择的稀疏性，防止过拟合。     |

模型示意图：

![model](./model.png)

训练使用了动量下降法，设置监听器检查验证集的损失值是否在持续下降，监听周期为10个`epoch`，训练配置信息如下：

```python
import tensorflow as tf

from process_data import train_ds, test_ds
from models import build_model

model = build_model()
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-3),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                              patience=10,
                                              restore_best_weights=True)]

model.fit(train_ds,
          validation_data=test_ds,
          epochs=100,
          callbacks=callbacks,
          verbose=2)

model.save("../模型/results(2)")
```

## 训练过程

运行train文件，可训练并保存模型。

```
cd 源码
python train.py
```

在`源码`文件夹下方有`training.log`文件，记录了我的训练日志

## 预测过程

运行test文件，加载训练好的模型并生成预测结果。

```
cd 源码
python test.py
```

然后可以在`提交结果`文件夹下方找到`submission.csv`的提交文件
